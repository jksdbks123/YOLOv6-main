{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, torch, math, cv2\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from yolov6.utils.events import LOGGER, load_yaml\n",
    "from yolov6.layers.common import DetectBackend\n",
    "from yolov6.data.data_augment import letterbox\n",
    "from yolov6.utils.nms import non_max_suppression\n",
    "from yolov6.core.inferer import Inferer\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_divisible( x, divisor):\n",
    "        # Upward revision the value x to make it evenly divisible by the divisor.\n",
    "    return math.ceil(x / divisor) * divisor\n",
    "\n",
    "def check_img_size(img_size, s=32, floor=0):\n",
    "\n",
    "    \"\"\"Make sure image size is a multiple of stride s in each dimension, and return a new shape list of image.\"\"\"\n",
    "    if isinstance(img_size, int):  # integer i.e. img_size=640\n",
    "        new_size = max(make_divisible(img_size, int(s)), floor)\n",
    "    elif isinstance(img_size, list):  # list i.e. img_size=[640, 480]\n",
    "        new_size = [max(make_divisible(x, int(s)), floor) for x in img_size]\n",
    "    else:\n",
    "        raise Exception(f\"Unsupported type of img_size: {type(img_size)}\")\n",
    "\n",
    "    if new_size != img_size:\n",
    "        print(f'WARNING: --img-size {img_size} must be multiple of max stride {s}, updating to {new_size}')\n",
    "    return new_size if isinstance(img_size,list) else [new_size]*2\n",
    "\n",
    "device:str = \"gpu\"#@param [\"gpu\", \"cpu\"]\n",
    "half:bool = False #@param {type:\"boolean\"}\n",
    "cuda = device != 'cpu' and torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = r'D:/TrafficSign/YOLOv6-main/runs/train/exp2/weights/best_ckpt.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from D:/TrafficSign/YOLOv6-main/runs/train/exp2/weights/best_ckpt.pt\n",
      "\n",
      "Fusing model...\n",
      "C:\\Users\\zhChe\\Anaconda3\\envs\\tracking\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "model = DetectBackend(checkpoint, device=device)\n",
    "stride = model.stride\n",
    "class_names = load_yaml(\"./data/dataset.yaml\")['names']\n",
    "img_size:int = 1280#@param {type:\"integer\"}\n",
    "#@title Run YOLOv6 on an image from a URL. { run: \"auto\" }\n",
    "hide_labels: bool = False #@param {type:\"boolean\"}\n",
    "hide_conf: bool = False #@param {type:\"boolean\"}\n",
    "\n",
    "conf_thres: float =.25 #@param {type:\"number\"}\n",
    "iou_thres: float =.45 #@param {type:\"number\"}\n",
    "max_det:int =  1000#@param {type:\"integer\"}\n",
    "agnostic_nms: bool = False #@param {type:\"boolean\"}\n",
    "\n",
    "img_size = check_img_size(img_size, s=stride)\n",
    "\n",
    "if half & (device.type != 'cpu'):\n",
    "    model.model.half()\n",
    "else:\n",
    "    model.model.float()\n",
    "    half = False\n",
    "\n",
    "if device.type != 'cpu':\n",
    "    model(torch.zeros(1, 3, *img_size).to(device).type_as(next(model.model.parameters())))  # warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r'D:\\TrafficSign\\custom_dataset\\images\\test'\n",
    "image_path_list = os.listdir(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_src = cv2.imread(os.path.join(image_path,'road822.jpg'))\n",
    "# if image_src is None:\n",
    "#     continue\n",
    "image_ori = image_src.copy()\n",
    "image = letterbox(image_src, img_size, stride=stride)[0]\n",
    "image = image.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
    "\n",
    "image = torch.from_numpy(np.ascontiguousarray(image))\n",
    "image = image.half() if half else image.float()  # uint8 to fp16/32\n",
    "image /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "image = image.to(device)\n",
    "if len(image.shape) == 3:\n",
    "    image = image[None]\n",
    "pred_results = model(image)\n",
    "classes:Optional[List[int]] = None # the classes to keep\n",
    "det = non_max_suppression(pred_results, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)[0]\n",
    "gn = torch.tensor(image_src.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "if len(det)>0:\n",
    "    det[:, :4] = Inferer.rescale(image.shape[2:], det[:, :4], image_src.shape).round()\n",
    "#     info = det[:,4:].cpu().detach().numpy()\n",
    "#     image_name = os.path.join(post_image_folder,image_path_list[i])\n",
    "#     imgname = np.array(len(info) * [os.path.abspath(image_name)])\n",
    "#     infos.append(info)\n",
    "#     names.append(imgname)\n",
    "    for *xyxy, conf, cls in reversed(det):\n",
    "        class_num = int(cls)\n",
    "        label = None if hide_labels else (class_names[class_num] if hide_conf else f'{class_names[class_num]} {conf:.2f}')\n",
    "        Inferer.plot_box_and_label(image_ori, max(round(sum(image_ori.shape) / 2 * 0.003), 2), xyxy, label, color=Inferer.generate_colors(class_num, True))\n",
    "#     cv2.imwrite(image_name,image_ori)\n",
    "image_ori = cv2.resize(image_ori,(1280,1280))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow('a',image_ori)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Copy of Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
